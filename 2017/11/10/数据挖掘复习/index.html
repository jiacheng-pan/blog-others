<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>浙江大学-数据挖掘课程-复习笔记 | Others | 咸鱼Jackie</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. 介绍什么是数据挖掘：抽取interesting pattern数据挖掘的过程：knowledge discovery 过程KDD 可以被挖掘的pattern generalization(概括)  Information integration 信息聚合，数据仓库的构建（数据清洗、变换、聚合和多维数据模型） Data cube technology数据立方技术 Multidimensiona">
<meta name="keywords" content="数据挖掘,复习笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="浙江大学-数据挖掘课程-复习笔记">
<meta property="og:url" content="http://jackieanxis.github.io/2017/11/10/数据挖掘复习/index.html">
<meta property="og:site_name" content="Others | 咸鱼Jackie">
<meta property="og:description" content="1. 介绍什么是数据挖掘：抽取interesting pattern数据挖掘的过程：knowledge discovery 过程KDD 可以被挖掘的pattern generalization(概括)  Information integration 信息聚合，数据仓库的构建（数据清洗、变换、聚合和多维数据模型） Data cube technology数据立方技术 Multidimensiona">
<meta property="og:locale" content="Chinese">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/10999007.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/67206708.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/19846478.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/91244837.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/49705672.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/65227165.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/23023651.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/33167128.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/9919791.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/64012434.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-14/41219297.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-14/12962797.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-14/80220904.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-14/48564062.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-14/63996666.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/1487277.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/11954617.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/53320418.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/10773567.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/19614960.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/95693211.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-15/71303397.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-16/54708839.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-16/66390485.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-16/32057659.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-16/68799474.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-17/11929324.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-17/31501679.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-17/63923841.jpg">
<meta property="og:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-17/66309480.jpg">
<meta property="og:updated_time" content="2018-01-16T13:43:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="浙江大学-数据挖掘课程-复习笔记">
<meta name="twitter:description" content="1. 介绍什么是数据挖掘：抽取interesting pattern数据挖掘的过程：knowledge discovery 过程KDD 可以被挖掘的pattern generalization(概括)  Information integration 信息聚合，数据仓库的构建（数据清洗、变换、聚合和多维数据模型） Data cube technology数据立方技术 Multidimensiona">
<meta name="twitter:image" content="http://o6vut8vrh.bkt.clouddn.com/17-11-13/10999007.jpg">
  
  
    <link rel="icon" href="http://o6vut8vrh.bkt.clouddn.com/avatar.png">
  
  <link rel="stylesheet" href="/blog-others/css/typing.css">
  <link rel="stylesheet" href="/blog-others/css/donate.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
</head>

  
    
      <body>
    
  
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container" class="container">
        <article id="post-数据挖掘复习" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
    <div class="mobile-nav">
      <h1 class="nickname">Jackie Anxis</h1>
      <a id="menu">
        &#9776; Menu
      </a>
    </div>
    
        <nav id="main-nav" class="main-nav nav-left">
    
    
      <a class="main-nav-link" href="http://jackieanxis.github.io">Home</a>
    
      <a class="main-nav-link" href="http://jackieanxis.github.io/blog-frontend">FrontEnd</a>
    
      <a class="main-nav-link" href="http://jackieanxis.github.io/blog-cs229">MLcs229</a>
    
      <a class="main-nav-link" href="http://jackieanxis.github.io/blog-papers">PaperReading</a>
    
      <a class="main-nav-link" href="https://github.com/JackieAnxis">Github</a>
    
      <a class="main-nav-link" href="http://jackieanxis.github.io/blog-others">Others</a>
    
      <a class="main-nav-link" href="/blog-others/about">About</a>
    
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      浙江大学-数据挖掘课程-复习笔记
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="什么是数据挖掘：抽取interesting-pattern"><a href="#什么是数据挖掘：抽取interesting-pattern" class="headerlink" title="什么是数据挖掘：抽取interesting pattern"></a>什么是数据挖掘：抽取interesting pattern</h3><h3 id="数据挖掘的过程：knowledge-discovery-过程KDD"><a href="#数据挖掘的过程：knowledge-discovery-过程KDD" class="headerlink" title="数据挖掘的过程：knowledge discovery 过程KDD"></a>数据挖掘的过程：knowledge discovery 过程KDD</h3><p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/10999007.jpg" alt=""></p>
<h3 id="可以被挖掘的pattern"><a href="#可以被挖掘的pattern" class="headerlink" title="可以被挖掘的pattern"></a>可以被挖掘的pattern</h3><ol>
<li><p>generalization(概括)</p>
<ul>
<li>Information integration 信息聚合，数据仓库的构建（数据清洗、变换、聚合和多维数据模型）</li>
<li>Data cube technology数据立方技术</li>
<li>Multidimensional concept description多维概念描述（分类和识别）</li>
</ul>
</li>
<li><p>association and correlation analysis(关联分析和相关分析)</p>
<ul>
<li>发掘Frequent pattern</li>
<li>association correlation vs causality（关联，相关和因果关系）</li>
</ul>
</li>
<li><p>classification(分类)</p>
<ul>
<li>建立基于训练样本的模型</li>
<li>描述，区分不同的类别</li>
<li>预测一些未知的类别标记</li>
</ul>
</li>
<li><p>cluster analysis(聚类)</p>
<ul>
<li>无监督学习（比如：不知道类别标签）</li>
<li>将结果进行分类成不同的类别</li>
<li>原则：最大化类内的相似度，并且最小化类间相似度</li>
</ul>
</li>
<li><p>outlier analysis(离群点分析)</p>
<ul>
<li>outlier(离群点)：指的是不符合数据一般表现的数据个体</li>
<li>噪音？异常？</li>
<li>方法：聚类、回归分析</li>
</ul>
</li>
<li><p>Time and Ordering: Sequential Pattern, Trend and Evolution Analysis（时序数据，趋势分析和演变分析）</p>
<ul>
<li>Sequence, trend and evolution analysis（序列，趋势和演化分析）</li>
<li>挖掘数据流</li>
</ul>
</li>
<li><p>Structure and Network Analysis(结构分析和网络分析)</p>
<ul>
<li>graph mining：图数据挖掘</li>
<li>web mining：网络数据挖掘</li>
<li>信息网络分析</li>
</ul>
</li>
</ol>
<h3 id="数据挖掘的主要问题"><a href="#数据挖掘的主要问题" class="headerlink" title="数据挖掘的主要问题"></a>数据挖掘的主要问题</h3><ol>
<li>挖掘方法<ul>
<li>挖掘多种不同的知识</li>
<li>在多维空间中挖掘知识</li>
<li>跨学科</li>
<li>提高在网络环境中挖掘数据的能力</li>
<li>噪声、不确定性、数据的不完整性</li>
<li>pattern演变</li>
<li>有约束条件的挖掘</li>
</ul>
</li>
<li>用户交互（和领域背景知识的结合）</li>
<li>可视化（Efficiency and Scalability高效、可扩展）</li>
<li>数据类型的多变性</li>
<li>数据挖掘与社会影响</li>
</ol>
<h2 id="2-数据"><a href="#2-数据" class="headerlink" title="2. 数据"></a>2. 数据</h2><h3 id="数据对象和属性类型"><a href="#数据对象和属性类型" class="headerlink" title="数据对象和属性类型"></a>数据对象和属性类型</h3><ol>
<li>数据集的类型<ul>
<li>记录（record）：关系记录，矩阵，文档数据，交易数据</li>
<li>图和网络（graph and network）</li>
<li>有序数据（ordered）：视频、时序数据、基因序列数据</li>
<li>空间、图像和多媒体</li>
</ul>
</li>
<li>结构化数据的重要特征：<ul>
<li>维度（dimensionality）</li>
<li>稀疏度（sparsity)</li>
<li>分辨率（resolution）</li>
<li>分布（distribution）</li>
</ul>
</li>
<li>数据对象<ul>
<li>一个数据对象代表一个实体</li>
<li>也被叫做samples, examples, instances, data points, objects, tuples</li>
<li>数据对象用属性来表述</li>
<li>rows：数据对象；columns：属性</li>
</ul>
</li>
<li>属性（Attribute or dimensions, features, variables）<ul>
<li>nominal：枚举属性（类别数据），类别，状态，是可数的，比如Hair_color = {auburn, black, blond, brown, grey, red, white}</li>
<li>ordinal：序数属性（有序数据），属性值有一个有意义的顺序，但相邻两级之间的差距是未知的</li>
<li>binary：二元属性<ul>
<li>对称二元属性（等价，同权，比如男女）</li>
<li>非对称二元属性（不等价，如艾滋病毒的阴性和阳性，将重要的（往往是稀有的）编码为1）</li>
</ul>
</li>
<li>numeric：数值属性<ul>
<li>数值（quantity）</li>
<li>区间属性（interval）：用相等的单位尺度的单元来表示，而且值是有序的。没有绝对的零值，并没有倍数关系（比如不能说10℃是5℃的两倍温暖）</li>
<li>比率属性（ratio）：有零值</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="数据的基本统计描述"><a href="#数据的基本统计描述" class="headerlink" title="数据的基本统计描述"></a>数据的基本统计描述</h3><ul>
<li><p>中位数，最大值，最小值，分位数，离群值，方差等等（median, max, min, quantiles, outliers, variance, etc.）</p>
<ul>
<li>mean，均值（代数意义上的）</li>
<li>mode，众数，可能有多个众数</li>
<li>median，中位数</li>
</ul>
</li>
<li><p>对称的和倾斜的数据：对称，正倾斜（众数小于中位数），负倾斜（众数大于中位数）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/67206708.jpg" alt=""></p>
</li>
<li><p>分位数，离群值和盒须图</p>
<ul>
<li>分位数（Quartiles）：Q1（25分位数），Q3（75分位数）</li>
<li>四分位数间距（inter-quartile range），IQR=Q3-Q1</li>
<li>盒须图的五个点：min, Q1, median, Q3, max</li>
</ul>
</li>
<li><p>方差和标准差</p>
<ul>
<li><p>有偏估计方差</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/19846478.jpg" alt=""></p>
</li>
<li><p>无偏估计方差</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/91244837.jpg" alt=""></p>
</li>
</ul>
</li>
<li><p>可视化</p>
<ul>
<li><p>盒须图</p>
</li>
<li><p>统计直方图</p>
</li>
<li><p>分位数图（Quantile Plot），横轴是百分比，纵轴是数值</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/49705672.jpg" alt=""></p>
</li>
<li><p>Q-Q Plot，比较两组数据是否来自同一分布</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/65227165.jpg" alt=""></p>
</li>
<li><p>散点图（Scatter plot）</p>
</li>
</ul>
</li>
</ul>
<h3 id="数据相似度和相异度"><a href="#数据相似度和相异度" class="headerlink" title="数据相似度和相异度"></a>数据相似度和相异度</h3><ul>
<li><p>数据矩阵：n*p矩阵，n是数据对象个数，p是属性个数。</p>
</li>
<li><p>相异度矩阵：n*n矩阵</p>
</li>
<li><p>枚举属性（nominal attribute）的相异度度量：</p>
<ul>
<li>简单的匹配，相异度d(i,j)=(p-m)/p，p是属性个数，m是匹配的属性</li>
<li>将枚举属性转换成二元属性（比如color={red, green, blue}，可以转换成三个二元属性），用二元属性的相异度度量</li>
</ul>
</li>
<li><p>二元属性（binary attribute）的相异度度量：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/23023651.jpg" alt=""></p>
<ul>
<li>对称属性的距离：$d=(r+s)/(q+r+s+t)$</li>
<li>非对称属性的距离：$d=(r+s)/(q+r+s)$</li>
<li>Jaccard相关系数（非对称属性的相似度度量）：$sim=(q)/(q+r+s)$</li>
</ul>
</li>
<li><p>数值属性（numeric)：</p>
<ul>
<li><p>标准化：</p>
<ul>
<li><p>Z-score: $z=(x-\mu)/\sigma$，$\mu$是平均值，$\sigma$是标准差</p>
</li>
<li><p>平均绝对离差（mean absolute deviation）：计算每个属性的平均值，以及每个属性的标准差，再进行z-score标准化，更鲁棒</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/33167128.jpg" alt=""></p>
</li>
</ul>
</li>
<li><p>欧几里得距离（Euclidean Distance）：$d=\sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+…+(x_{ip}-x_{jp})^2}$</p>
</li>
<li><p>曼哈顿距离$d=|x_{i1}-x_{j1}|+|x_{i2}-x_{j2}|+…+|x_{ip}-x_{jp}|$</p>
</li>
<li><p>闵可夫斯基距离（Minkowski distance）：$d=\sqrt[h]{|x_{i1}-x_{j1}|^h+|x_{i2}-x_{j2}|^h+…+|x_{ip}-x_{jp}|^h}$</p>
<p>范数</p>
</li>
<li><p>上确界距离（$L_{max}$，切比雪夫距离）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/9919791.jpg" alt=""></p>
</li>
</ul>
<p>​</p>
</li>
<li><p>有序属性（ordinal）</p>
<ul>
<li><p>标准化，映射到[0,1]，$r_{if}$是原始值的排序值，$M_f$是属性$f$的状态数</p>
<p>$z_{if}=\frac{r_{if}-1}{M_{f}-1}$</p>
</li>
<li><p>用数值属性提到的四种距离来计算</p>
</li>
</ul>
</li>
<li><p>混合类型</p>
<ul>
<li><p>将所有属性，映射到共同的区间[0,1]</p>
</li>
<li><p>计算距离：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-13/64012434.jpg" alt=""></p>
<ul>
<li>指示符$\delta_{ij}=0$ 表示，对象i或对象j缺少属性f,或者$x_{if} = x_{jf} = 0$且f是非对称的二元属性；否则为1。</li>
<li>$d_{ij}^{(f)}$，表示i和j在属性f上的距离：<ul>
<li>二元属性或者枚举属性：相同为0，不同为1；</li>
<li>有序属性，用有序属性标准化的方式进行标准化</li>
<li>数值属性，两者的差/（属性f的最大值-属性f的最小值）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>余弦相似度（Cosine Similarity），一般用于计算文档，每个文档都有一个词频向量。</p>
<p>cos(d1, d2) = (d1· d2) /||d1|| ||d2|| ，两个向量之间的余弦值，||x||是x的欧几里得范式</p>
</li>
</ul>
<h2 id="3-数据处理"><a href="#3-数据处理" class="headerlink" title="3. 数据处理"></a>3. 数据处理</h2><h3 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a>数据质量</h3><ul>
<li><strong>准确性（accuracy）</strong></li>
<li><strong>完备性（completeness）</strong></li>
<li><strong>一致性（consistency）</strong>，有些修改了，有些没修改</li>
<li>时效性（timeliness）</li>
<li>可信性（believability）</li>
<li>可解释性（interpretability）</li>
</ul>
<h3 id="数据处理的主要任务"><a href="#数据处理的主要任务" class="headerlink" title="数据处理的主要任务"></a>数据处理的主要任务</h3><ul>
<li>数据清洗（datac cleaning）：填补缺失值，平滑噪声，去除异常值，解决不一致性</li>
<li>数据集成（data integration）：将多源数据进行集成</li>
<li>数据简化（data reduction）：降维、数量归约（使用回归，聚类等方法，用较小的表示取代数据）、数据压缩</li>
<li>数据变换和离散化（Data transformation and data discretization），进行标准化</li>
</ul>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><ol>
<li>缺失值<ul>
<li>忽略</li>
<li>手动</li>
<li>添加为新的类别，比如unknown</li>
<li>用平均值或者中位数来填充</li>
<li>用同一类的样本的均值或者中位数来填充</li>
<li>最有可能的值：贝叶斯形式化方法（Bayesian formula）或者决策树</li>
</ul>
</li>
<li>噪声<ul>
<li>分箱，划分成等频率的箱，用箱的均值或者中位数，或者最近边界来平滑数据</li>
<li>回归</li>
<li>聚类：检测并去除离群值</li>
<li>人机合作</li>
</ul>
</li>
<li>不一致性（如何检测？）<ul>
<li>用元数据（定义域，值域，分布等）</li>
<li>字段过载(field overloading)，用了其他属性的未使用的部分的位置</li>
<li>检查唯一性规则（每个值都应该不同），连续性规则（最低和最高之间没有确缺失值），空值规则</li>
<li>使用商业工具</li>
</ul>
</li>
<li>伪造</li>
</ol>
<h3 id="数据集成"><a href="#数据集成" class="headerlink" title="数据集成"></a>数据集成</h3><p>多源数据的结合：模式集成（schema integration， e.g. nA.cust-id = B.cust-#），个体识别（entity identification，识别有不同名称的相同的个体），检测和解决数据值冲突。</p>
<ul>
<li><p>数据集成中的冗余（redundancy）问题</p>
<ul>
<li><p>两种冗余：同一个属性或者对象有着不同的名称；可被推导出来的值</p>
</li>
<li><p>可以通过相关分析（correlation analysis）和协方差分析（covariance analysis）进行冗余检测</p>
<ul>
<li><p>相关分析：$\chi^2$卡方检验</p>
<p>$\chi^2=\sum\frac{(Observed-Expected)^2}{Expected}$</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-14/41219297.jpg" alt=""></p>
<p>括号中的是它的期望值，比如，90=450*300/(300+1200)，于是</p>
<p>$\chi^2=\frac{(250-90)^2}{90}+\frac{(50-120)^2}{210}+\frac{(200-360)^2}{360}+\frac{(1000-840)^2}{840}$</p>
<p>卡方越大越相关。</p>
</li>
<li><p>相关分析：皮艾森系数（Pearson’s product moment coefficient）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-14/12962797.jpg" alt=""></p>
<p>​</p>
</li>
<li><p>协方差分析：针对数值型数据</p>
<p>协方差：<img src="http://o6vut8vrh.bkt.clouddn.com/17-11-14/80220904.jpg" alt=""></p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-14/48564062.jpg" alt=""></p>
<p>协相关系数（correlation coefficient:）：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-14/63996666.jpg" alt=""></p>
<p>协方差为正，说明A B趋向于一起改变，A大于期望的时候，B也很可能大于它的期望</p>
<p>协方差为负，说明当一个属性小于它的期望，另一个则趋向于比期望更大</p>
<p>协方差为0，说明两者独立，因为$E(A · B) = E(A)·E(B)$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="数据简化（reduction）"><a href="#数据简化（reduction）" class="headerlink" title="数据简化（reduction）"></a>数据简化（reduction）</h3><ul>
<li><p>降低维度（Dimensionality Reduction）</p>
<ul>
<li><p>动机：维数灾难，当维度增加，数据变得稀疏，</p>
</li>
<li><p>方法：</p>
<ol>
<li><p>小波变换（wavelet transforms）</p>
<ul>
<li><p>将一个信号分解为不同频率的子带，保留数据对象之间的相对距离，只保留一小部分小波系数最强的信息，和傅立叶变换类似，但空间局部性更好，有助于保留局部细节</p>
</li>
<li><p>为什么选择小波变换</p>
<p>有效移除离群值，多分辨率（在不同缩放率下都可以检测任意形状的聚类），高效（时间复杂度O(N)），但只适用于低维数据</p>
</li>
</ul>
</li>
<li><p>PCA主成分分析</p>
<p>找出k个最能代表数据的n维正交向量（k&lt;=n），也就是找到一个投影能够捕捉到数据中最主要的变换。</p>
<ul>
<li>先标准化输入数据，使得所有属性都投影到同一区间。</li>
<li>计算K个标准正交向量，这些向量作为规范化输入数据的基，称为主成分。输入数据即为主成分的线性组合</li>
<li>对于主成分，按照重要程度或者强度进行排序</li>
<li>去掉排序靠后的，不重要的，方差较小的那些正交向量</li>
</ul>
</li>
<li><p>属性子集选择（attribute subset selection）</p>
<p>通过删除不相关或者冗余的属性来减少数据量。</p>
<p>启发式搜索（贪心算法），属性的好坏，可以用统计显著性检验来确定</p>
<ul>
<li>逐步选择：每次从属性集里选出一个最好的属性，添加到目标集合中</li>
<li>逐步删除：每次从属性集中删除一个最差的属性</li>
<li>两者结合：每次都选出一个最好属性，并删除一个最差的</li>
</ul>
<p>​</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p>简化数量（Numerosity Reduction）</p>
<ul>
<li><p>参数化方法</p>
<p>假设数据会符合某些模型，这样就可以只记录模型参数，忽略数据（x，y表示数值属性）</p>
<ul>
<li>线性回归：简单直线（$y=wx+b$）</li>
<li>多元回归：用多个自变量的线性函数对因变量Y进行建模（$y=b_0+b_1x_1+b_2x_2+…+b_kx_k$）</li>
<li>对数线性模型：对于离散属性值，可以用对数线性模型，基于维组合的一个较小子集，估计多维空间中每个点的概率。</li>
</ul>
</li>
<li><p>非参数化方法</p>
<p>未假设模型的存在</p>
<ul>
<li>直方图：等宽分割（宽度接近）和等频分割（高度接近）</li>
<li>聚类</li>
<li>采样<ol>
<li>无放回简单随机采样</li>
<li>有放回简单随机采样</li>
<li>分层抽样（stratified sampleing）：分割数据集。对倾斜数据比较有效</li>
</ol>
</li>
<li>数据立方聚集</li>
</ul>
</li>
</ul>
</li>
<li><p>数据压缩（Data Compression）</p>
<ul>
<li>字符串压缩</li>
<li>音频/视频压缩</li>
</ul>
</li>
</ul>
<h3 id="数据变换和数据离散化"><a href="#数据变换和数据离散化" class="headerlink" title="数据变换和数据离散化"></a>数据变换和数据离散化</h3><ul>
<li><p>数据变换</p>
<ol>
<li><p>光滑（去除噪声）</p>
</li>
<li><p>属性构造（ 由已有的属性构造出新属性添加到属性集中）</p>
</li>
<li><p>聚集（汇总）</p>
</li>
<li><p>规范化（标准化）</p>
<ul>
<li><p>min-max，标准化到[new_min, new_max]</p>
<p>$v’=\frac{v-min}{max-min}*(new_max-new_min)+new_min$</p>
</li>
<li><p>z-score</p>
<p>$v’=\frac{v-\mu}{\sigma}$</p>
</li>
<li><p>小数定标 decimal scaling</p>
<p>$v’=\frac{v}{10^j}$，其中j是使得v’最大绝对值小于1的最小的整数</p>
</li>
</ul>
</li>
<li><p>离散化</p>
</li>
</ol>
</li>
<li><p>离散化</p>
<ol>
<li>分箱：无监督，自顶向下分裂，指定箱的个数；容易受离群值影响；有等宽和等深频</li>
<li>直方图：无监督，自顶向下分裂，等宽/等频</li>
<li>聚类：无监督，自顶向下分裂/自下向上合并</li>
<li>决策树：有监督，自顶向下分裂。</li>
<li>相关性分析。有监督，自下向上合并</li>
</ol>
<ul>
<li><p>概念分层</p>
<ul>
<li><p>通过用户或专家，显式的说明部分或者所有的属性层次序列</p>
</li>
<li><p>通过显示数据分组，说明分层结构的一部分，比如定义{浙江，江苏，福建}属于华东地区</p>
</li>
<li><p>自动根据每个属性的不同值个数产生概念分层</p>
<p>​</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-数据仓库和联机分析处理"><a href="#4-数据仓库和联机分析处理" class="headerlink" title="4. 数据仓库和联机分析处理"></a>4. 数据仓库和联机分析处理</h2><h2 id="5-数据立方技术"><a href="#5-数据立方技术" class="headerlink" title="5. 数据立方技术"></a>5. 数据立方技术</h2><h2 id="6-挖掘频繁模式、关联和相关性"><a href="#6-挖掘频繁模式、关联和相关性" class="headerlink" title="6. 挖掘频繁模式、关联和相关性"></a>6. 挖掘频繁模式、关联和相关性</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ol>
<li><p>动机：找到数据的内在规律</p>
</li>
<li><p>项集（itemset）</p>
</li>
<li><p>事务（transaction），为一个非空项集</p>
</li>
<li><p>频度（frequency），</p>
</li>
<li><p>关联规则（association rules），X=&gt;Y，X，Y是两个不相交的非空项集。</p>
</li>
<li><p>强关联规则：支持度和置信度都高于阈值</p>
</li>
<li><p>支持度（support）：包含$X \cup Y$的事务的出现概率</p>
</li>
<li><p>置信度（confidence）：包含X的事务同时也包含Y的概率，P(Y|X)</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/1487277.jpg" alt=""></p>
<p>$support(Beer \Rightarrow Diaper)=count(10,20,30) / 5 = 60\%, confidence(Beer \Rightarrow Diaper)=count(10,20,30) / count(10,20,30) = 100\%$</p>
<p>$support(Diaper \Rightarrow Beer)=count(10,20,30) / 5 = 60\%, confidence(Diaper \Rightarrow Beer)=count(10,20,30) / count(10,20,30,50) = 75\%$</p>
</li>
<li><p>因为长项集的子项集组合过多，比如包含100项的相机，它的子集组合就有$2^{100}-1$个。故而把问题转换成挖掘其中的闭频繁项集和极大频繁项集：</p>
<ul>
<li>closed pattern：如果不存在X的真超项集Y，使得Y和X在数据集D中有着相同的频度，那么称X为闭的（closed）</li>
<li>Max-Patterns：如果X是频繁的，且不存在X的超项集Y，并且Y是频繁的</li>
</ul>
</li>
<li><p>时间复杂度：最坏情况$M^N$，M是不同的项个数，N是事务的最大长度</p>
</li>
</ol>
<p>### </p>
<h3 id="Apriori算法："><a href="#Apriori算法：" class="headerlink" title="Apriori算法："></a>Apriori算法：</h3><p>基于一个先验性质，频繁项集的所有非空子集也一定是频繁的，反而言之，如果一个项集是不频繁的，那么它的任何超集也都不用再进行验证。</p>
<ul>
<li>第一次生成一项集，排除里面支持度小于阈值的项集。</li>
<li>根据上一次生成的项集，形成N+1项集</li>
<li>排除N+1项集中，支持度小于阈值的项集，重复上一步，直到所有项集的支持度都低于阈值</li>
</ul>
<p>案例：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/11954617.jpg" alt=""></p>
<h3 id="提高Apriori算法："><a href="#提高Apriori算法：" class="headerlink" title="提高Apriori算法："></a>提高Apriori算法：</h3><ul>
<li>问题：多次扫描数据库中的事务，庞杂的候选项，计数候选项支持度的开销大</li>
<li>解决：<ol>
<li>划分（partition）：只需要两次扫描数据库。第一次，将数据库D中的事务，划分成n个非重叠分区，计算每个分区的局部最小支持度计数阈值（min_sup * 分区事务个数）。若项集超过这个局部最小支持度计数，那么认为这个项集是局部频繁项集。全局的频繁项集一定出现在局部频繁项集中，故而将局部频繁项集作为D的候选项集；第二次，再次扫描D，评估候选项集的实际支持度，删除低于阈值的项集。</li>
<li>散列（hash，DHP）：利用散列哈希，如果哈希结束后，桶中的项集个数比阈值还小，那么这个桶中的项集就一定会被淘汰。而桶中项集个数大于阈值，也不一定就是频繁项集。</li>
<li>采样（sampling）：牺牲精度换取可行性。利用D的一个采样S，找出S中的频繁项集（故而阈值也会重新计算，此时的频繁是相对S的频繁），但S中的频繁项集并不一定是D中的频繁项集，会有丢失。故而要用低于最小支持度的阈值来搜索S，从而提高精度。</li>
<li>动态项集计数（DIC）</li>
</ol>
</li>
</ul>
<h3 id="模式增长方法（Pattern-Growth-Approach）"><a href="#模式增长方法（Pattern-Growth-Approach）" class="headerlink" title="模式增长方法（Pattern-Growth Approach）"></a>模式增长方法（Pattern-Growth Approach）</h3><ol>
<li><p>构建fp tree：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/53320418.jpg" alt=""></p>
</li>
<li><p>条件模式基（conditional pattern bases）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/10773567.jpg" alt=""></p>
</li>
<li><p>寻找条件模式树（conditional FP-tree，类似于寻找最长公共子序列）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/19614960.jpg" alt=""></p>
</li>
<li><p>简化，前缀可以被简化成一个节点</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/95693211.jpg" alt=""></p>
</li>
<li><p>fp-tree的优势</p>
<ul>
<li>完全性（completeness）：包含了fp mining需要的所有信息，不拆分任何一个事务的长pattern</li>
<li>紧密型（compactness）：去除了无关信息（非频繁的项被省去）；高频项被放在前面；不会比初始数据库更大</li>
</ul>
</li>
<li><p>挖掘方法</p>
<ul>
<li>对每个频繁项，构造它的条件模式基（conditional pattern-base），进而构造它的条件频繁模式树（conditional FP-tree）</li>
<li>对每个conditional FP-tree，重复以上步骤</li>
<li>直到FP-tree是空的，或者只有一条路径（单一路径的所有子路径，组成了频繁模式）</li>
</ul>
</li>
<li><p>分割投影</p>
<p>为了让fp-tree能放进主存，需要将数据库划分成投影数据库的集合。比如4图中，就可以先分成两个以r1为前缀项集的投影数据库${b1}，{ {c1,c2},{c1,c3}}$</p>
</li>
</ol>
<h3 id="用等价类变换（ECLAT）进行垂直数据格式挖掘"><a href="#用等价类变换（ECLAT）进行垂直数据格式挖掘" class="headerlink" title="用等价类变换（ECLAT）进行垂直数据格式挖掘"></a>用等价类变换（ECLAT）进行垂直数据格式挖掘</h3><p>前面的方法都是TID项集格式的挖掘方式，这种数据格式称为水平数据格式；而垂直数据格式刚好是它的一个转置。$t(X)= {T1,T2,T3},  t(XY) = {T1, T3}$</p>
<p>加速：比如上方，$Diffset(XY,X)={T2}$，这样就不用记录$t(XY)$的两个项，而只要存储差集的一个项就行了。</p>
<h3 id="挖掘闭频繁模式和极大模式"><a href="#挖掘闭频繁模式和极大模式" class="headerlink" title="挖掘闭频繁模式和极大模式"></a>挖掘闭频繁模式和极大模式</h3><ol>
<li><p>挖掘闭模式</p>
<ul>
<li><strong>项合并</strong>：如果包含频繁项集$X$的事务都包含$Y$，且不包含$Y$的任何真超集，那么$X \cup Y$形成一个闭频繁项集，并且可以不再搜索包含$X$且不包含$Y$的任何项集。</li>
<li><strong>子项集剪枝</strong>：如果X是一个已发现的闭频繁项集Y的真子集，且$support_count(X)=support_count(Y)$(说明，X没有单独出现在任何事务中)，那么X和X的子集都不可能是闭频繁项集</li>
<li><strong>项跳过</strong>：在头表不同层，某个局部频繁项都有着一样的support，那么这一项就可以从头表中删除</li>
</ul>
</li>
<li><p>挖掘极大模式</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-15/71303397.jpg" alt=""></p>
</li>
</ol>
<h3 id="找出interesting-pattern"><a href="#找出interesting-pattern" class="headerlink" title="找出interesting pattern"></a>找出interesting pattern</h3><p>强关联规则并不一定准确。需要其他度量</p>
<ul>
<li><p>correlations（相关性）。</p>
<ul>
<li><p>提升度lift</p>
<p>假如A项集和B项集的出现是独立事件，那么$P(A \cup B) = P(A)P(B)$；否则，两者相互依赖（dependent）和相关（correlated），可以通过提升度（lift）来表示</p>
<p>$lift(A, B) = \frac{P(A \cup B)}{P(A)P(B)}=\frac{P(B|A)}{P(B)}$</p>
<p>如果提升度小于1，说明发生A时发生B的概率，比光是发生B的概率要小，A和B负相关；</p>
<p>如果提升度大于1，说明发生A时发生B的概率，比光是发生B的概率还大，A和B正相关；</p>
<p>如果提升度等于1，说明发生A时发生B的概率，和光是发生B的概率一致，说明两者无关独立。</p>
</li>
<li><p>卡方$\chi^2$</p>
</li>
</ul>
</li>
<li><p>不平衡比（Imbalance Ratio）</p>
<p>$IR(A, B) = \frac{|sup(A)-sup(B)|}{sup(A)+sup(B)-sup(A \cup B)}$</p>
<p>分子是支持度之差的绝对值；分母是包含A或B的事务个数。越大越不平衡。</p>
</li>
</ul>
<h2 id="7-高级模式挖掘"><a href="#7-高级模式挖掘" class="headerlink" title="7. 高级模式挖掘"></a>7. 高级模式挖掘</h2><h2 id="8-分类"><a href="#8-分类" class="headerlink" title="8. 分类"></a>8. 分类</h2><p>预测问题包括分类和数值预测。</p>
<h3 id="分类：一个两步过程"><a href="#分类：一个两步过程" class="headerlink" title="分类：一个两步过程"></a>分类：一个两步过程</h3><ol>
<li>模型建构<ul>
<li>每个样本都假设属于一个预定义的类</li>
<li>模型可能表现为：分类规则，决策树或者数学公式</li>
</ul>
</li>
<li>模型使用<ul>
<li>评价模型精确程度</li>
<li>假如精确度可接受，那么就可以用来标记新数据</li>
<li>监督学习和无监督学习<ul>
<li>监督学习：训练数据是有标记的</li>
<li>无监督学习：训练数据无标记，目标是进行聚类</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="决策树归纳"><a href="#决策树归纳" class="headerlink" title="决策树归纳"></a>决策树归纳</h3><ul>
<li><p>决策树的构建算法：自顶向下的递归分治算法</p>
<ol>
<li>一开始所有训练样本都在根节点上，所有的属性都是有类别的（假如是连续的，需要提前离散化）</li>
<li>基于参数中给定的分裂准则，用选定的属性对样本进行划分，不断迭代</li>
<li>直到满足以下任一条件：<ul>
<li>给定节点中的所有样本都是同一类的</li>
<li>没有剩余的属性可以被用来做进一步分割</li>
<li>没有剩余的样本了</li>
</ul>
</li>
</ol>
</li>
<li><p>决策树构建中的分裂准则</p>
<ol>
<li><p>信息增益（Information Gain)</p>
<p>选择具有最高信息增益的属性作为节点N的分裂属性</p>
<ul>
<li><p>对D中的元组进行分类所需要的期望信息，也被称为D的熵：</p>
<p>$Info(D)=-\sum p_ilog_2(p_i)$</p>
<p>$p_i$是$D$中任意元组属于类$C_i$的概率（非0）</p>
</li>
<li><p>利用某个属性对D进行分区，得到的分区不一定是准确的分类，所以需要计算，要得到准确的分类，我们还需要多少信息：</p>
<p>$Info_A(D)=\sum \frac{|D_j|}{|D|} \times Info(D_j)$</p>
<p>其中$\frac{|D_j|}{|D|}$充当第j个分区的权重。$Info_A(D)$是基于A划分D所需要的期望信息，所需的期望信息越小，分区的纯度越高。</p>
</li>
<li><p>信息增益：</p>
<p>$Gain(A）=Info(D)-Info_A(D)$</p>
<p>选择最高信息增益的属性作为分裂属性，也就是说选择$Info_A(D)$最小。</p>
</li>
<li><p>计算连续值得的信息增益</p>
<p>A的值进行递增序排序，每对相邻的中值作为一个可能的分裂点（$(a_i+a_{i+1})/2$），对于A的给定的v个值，则需要计算v-1个可能的划分。</p>
<p>对每个分裂点计算$Info_A(D)$，对每个分裂点，分区个数是2，选出最小期望信息需求的点作为分裂点。</p>
</li>
</ul>
</li>
<li><p>增益率</p>
<p>$GainRate(A) = \frac{Gain(A)}{splitInfo_A(D)}$</p>
<p>其中，</p>
<p>$splitInfo_A(D)=-\sum \frac{|D_j|}{|D|} \times log_2(\frac{|D_j|}{|D|})$</p>
</li>
<li><p>基尼指数(Gini index)，针对二元分裂</p>
<ul>
<li><p>基尼指数，度量D的数据分区的不纯度：</p>
<p>$Gini(D) = 1 - \sum p_i^2$</p>
</li>
<li><p>利用属性A，将D划分为两个分区，从而得到的基尼指数：</p>
<p>$Gini_A(D)=\frac{|D_1}{|D|}Gini(D_1)+\frac{|D_2}{|D|}Gini(D_2)$</p>
</li>
<li><p>基尼指数下降：</p>
<p>$\Delta Gini(A)=Gini(D)-Gini_A(D)$</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>过拟合和剪枝</p>
<ul>
<li>因为噪声跟离群点的关系，有许多分支反映了训练数据中的一场，需要进行剪枝来处理这种过拟合的问题。</li>
<li>先剪枝和后剪枝<ul>
<li>先剪枝（prepruning），通过提前停止树的创建来剪枝</li>
<li>后剪枝（postpruning），删除节点的分支而用叶节点代替</li>
</ul>
</li>
</ul>
</li>
<li><p>大数据库的分类</p>
<ul>
<li><p>可伸缩的决策树算法，RainForest：</p>
<ul>
<li><p>AVG-set：在每个节点上，对每个属性都维护一个AVC-set。</p>
</li>
<li><p>AVC-group：节点上的所有AVC-set的集合。</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-16/54708839.jpg" alt=""></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="贝叶斯分类方法"><a href="#贝叶斯分类方法" class="headerlink" title="贝叶斯分类方法"></a>贝叶斯分类方法</h3><ol>
<li><p>贝叶斯定理</p>
<ul>
<li><p>先验概率，$P(H)$是H的先验概率</p>
</li>
<li><p>后验概率：$P(H|X)$是在条件X下，H的后验概率</p>
</li>
<li><p>贝叶斯定理：</p>
<p>$P(H|X)=\frac{P(X|H)P(H)}{P(X)}$</p>
</li>
</ul>
</li>
<li><p>朴素贝叶斯分类</p>
<ul>
<li><p>最大化$P(C_i|X)$：假定一个tuple用一个n维属性向量$X={x_1,x_2,…x_n}$表示，且假定有m个类，那么配件单贝叶斯分类法中，预测$X$属于$C_i$的概率为：$P(C_i|X)$，只要找到这个最大值对应的$C_i$即可。</p>
</li>
<li><p>最大化$P(X|C_i)$：而根据贝叶斯公式，只要找到$P(X|C_i)P(C_i)$的最大值即可。加入类的先验概率未知，我们通常假设所有类的先验概率一致，于是我们只要找到$P(X|C_i)$的最大值即可</p>
</li>
<li><p>假设$X$的各个属性之间相互独立，不存在依赖关系，那么</p>
<p>$P(X|C_i)=\prod P(x_k|C_i)$</p>
<ul>
<li><p>如果属性$x_k$是分类属性，那么概率即为训练集中属性值为$x_k$，且属于$C_i$的tuple在$C_i$中的比例</p>
</li>
<li><p>如果属性是连续值，一般假设属性服从高斯分布。</p>
<p>$P(x_k|C_i)=g(x_k,\mu_{C_i},\sigma_{C_i})$<img src="http://o6vut8vrh.bkt.clouddn.com/17-11-16/66390485.jpg" alt=""></p>
</li>
</ul>
</li>
<li><p>example</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-16/32057659.jpg" alt=""></p>
</li>
<li><p>关于0概率：拉普拉斯校准</p>
<p>假设训练集很大，对每个计数都加1，也不会对概率产生太大变化，从而避免0概率</p>
</li>
<li><p>优缺点</p>
<ul>
<li>优点：容易实现，在大部分情况下结果不错</li>
<li>缺点：基于分类条件独立假设，可以用贝叶斯信任网络来解决这个问题</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h3><ol>
<li><p>混淆矩阵：对于给定m个类，混淆矩阵至少是一个m*m的表。以下是一个2*2的混淆矩阵，纵向是实际分类，横向是预测分类。</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-16/68799474.jpg" alt=""></p>
<ul>
<li>准确率：$accuracy=(TP+TN)/(P+N)$</li>
<li>错误率：$error\ rate=(FP+FN)/(P+N)$</li>
<li>有些数据是不平衡的，比如在癌症检测，显然cancer=yes的元组才是我们关注的，于是有了以下两个度量：<ul>
<li>灵敏性（正确识别的正元组的比例）：$sensitivity=TP/P$，反映了识别正例的能力</li>
<li>特效性（正确识别的负元组的比例）：$specificity=TN/N$，反映了识别反例的能力</li>
</ul>
</li>
<li>精度（正确识别的正元组在预测为正元组中的比例）：$precision=(TP)/(TP+FP)$</li>
<li>召回率：$recall=TP/P$，其实也就是灵敏性</li>
<li>$F$度量：$F=(2 \times precision \times recall)/(precision+recall)$</li>
<li>$F_\beta$度量：$F_\beta=((1+\beta^2) \times precision \times recall)/(\beta^2 \times precision+recall)$</li>
</ul>
</li>
<li><p>保持方法和随机二次抽样</p>
<ul>
<li>保持方法（holdout）：将数据随机的分成训练集跟检验集（通常2/3作为训练集）</li>
<li>随机二次抽样（random subsampling）：将保持方法重复k次，结果取平均值。</li>
</ul>
</li>
<li><p>交叉验证（cross-validation）</p>
<ul>
<li>k折交叉验证（k-fold cross-validation），将数据随机分成k个相互不相交的子集（折），进行k次训练和检验。其中第i次迭代，用分区i作为检验集而用其余的作为训练集。准确率计算是用k次迭代的总数进行计算。</li>
</ul>
</li>
<li><p>自助法（bootstrap）</p>
<p>在小数据集下比较好。</p>
<ul>
<li><p>$.632$自助法：对于给定的包含d个元组的数据集，有放回抽样d次，产生d个样本的自主样本集或训练集，其余作为验证。平均情况下，63.2%的数据会被用于训练。</p>
<p>准确率计算：</p>
<p>$Acc(M)=\sum(0.632 \times Acc(M_i)_{test_set} + 0.368 \times Acc(M_i)_{train_set})$</p>
<p>$Acc(M_i)_{test_set}$是对于检验集i的准确率，$Acc(M_i)_{train_set})$是对于源数据的准确率</p>
</li>
</ul>
</li>
<li><p>选择模型的标准：</p>
<ul>
<li>准确率</li>
<li>速度</li>
<li>鲁棒性</li>
<li>可扩展性（对于大数据库的高效性）</li>
<li>可解释性</li>
</ul>
</li>
</ol>
<h3 id="提高分类准确率"><a href="#提高分类准确率" class="headerlink" title="提高分类准确率"></a>提高分类准确率</h3><ol>
<li>装袋（bagging）：对于不同的训练集Di（每个训练集都是一个自助样本）训练的分类模型Mi。为了对一个未知元组X进行分类，每个分类器Mi都会返回它的预测结果，算作投票中的一票，统计最终的票，将最高的得票赋予X。</li>
<li>提升（boosting）：迭代学习。初始所有训练集的元组权重都一致，每一轮迭代，提升上一次测试中出错的元组的权重，降低正确的元组的权重。</li>
<li>随机森林（random forest）</li>
</ol>
<h2 id="9-高级分类方法"><a href="#9-高级分类方法" class="headerlink" title="9. 高级分类方法"></a>9. 高级分类方法</h2><h3 id="惰性学习法"><a href="#惰性学习法" class="headerlink" title="惰性学习法"></a>惰性学习法</h3><ul>
<li><p>k-最近邻分类</p>
<p>​</p>
</li>
</ul>
<h2 id="10-聚类"><a href="#10-聚类" class="headerlink" title="10. 聚类"></a>10. 聚类</h2><h3 id="聚类质量"><a href="#聚类质量" class="headerlink" title="聚类质量"></a>聚类质量</h3><ol>
<li>高类内相似度，低类外相似度</li>
<li>聚类质量依赖于：相似度度量；聚类的实现；能够发掘隐藏pattern的能力</li>
<li>聚类质量的度量方法：相异度/相似度矩阵</li>
<li>聚类分析需要考量的因素：<ul>
<li>划分准则：单层划分和多层划分（互相之间有层级关系）</li>
<li>簇的分离性：互斥（一个客户只能属于一个组）和不互斥（一个文档可能有多个主题）</li>
<li>相似度度量：基于距离和基于连通性</li>
<li>聚类空间</li>
<li>可伸缩性</li>
<li>处理不同类型属性的能力</li>
<li>有约束条件的聚类</li>
</ul>
</li>
</ol>
<h3 id="主要聚类方法"><a href="#主要聚类方法" class="headerlink" title="主要聚类方法"></a>主要聚类方法</h3><ol>
<li><p>划分方法</p>
<ul>
<li>将数据划分成k个分区，保证每个分区最少有一个对象；例如k-means，k-medoids，CLARANS</li>
<li>发现球形互斥的簇</li>
<li>对中小规模数据集有效</li>
</ul>
</li>
<li><p>层次方法</p>
<ul>
<li>凝聚或者分裂的方法。层次聚类方法可以是基于距离或者密度和连通性的。</li>
<li>无法纠正错误的合并或划分</li>
</ul>
</li>
<li><p>基于密度的方法</p>
<p>基于对象之间的距离进行聚类，只能发现球状簇。主要思想：只要“邻域”中的密度超过某个阈值，就继续增长。对于给定簇中的每个数据点，在给定半径的邻域中至少包含最少数目的点。</p>
</li>
<li><p>基于网格的方法</p>
<p>用网格化的方法把对象空间量化为有限个单元。</p>
</li>
</ol>
<h3 id="划分方法"><a href="#划分方法" class="headerlink" title="划分方法"></a>划分方法</h3><p>一种度量簇质量的方法：</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-17/11929324.jpg" alt=""></p>
<p>$c_i$是簇$C_i$的代表（形心）</p>
<ol>
<li><p>k-means：局部最优，但不一定收敛到全局最优</p>
<p>将簇的形心定义为簇内点的均值。</p>
<ul>
<li>初始选取k个点，每个点代表一个簇的初始均值或中心。</li>
<li>其余点根据欧氏距离，分配给距离最近的簇。</li>
<li>更新迭代簇内均值，再分配。</li>
<li>直到不再变化。</li>
</ul>
<p>优点是高效；缺点是只在连续n维空间中有效，需要提前确定k，对噪声和离群值敏感，无法处理非凸形状的数据</p>
</li>
<li><p>k-medoids</p>
<p>将簇的形心定义为簇内某个实际的点。</p>
<ul>
<li>初始选取k个点，每个点代表一个簇的初始均值或中心。</li>
<li>其余点根据欧氏距离，分配给距离最近的簇。</li>
<li>随机选择一个非代表对象$O_{random}$代替$O_j$，观察绝对误差标准是否降低</li>
<li>如果降低，那么说明应该进行替换，并且重新形成簇</li>
<li>直到不再变化</li>
</ul>
<p>其中，绝对误差标准（absolute-erro criterion）的计算方法：如上。</p>
</li>
</ol>
<h3 id="层次方法"><a href="#层次方法" class="headerlink" title="层次方法"></a>层次方法</h3><h3 id="基于密度的方法"><a href="#基于密度的方法" class="headerlink" title="基于密度的方法"></a>基于密度的方法</h3><ul>
<li><p>主要特点：</p>
<ul>
<li>可以发现任意形状的簇</li>
<li>能应对噪声</li>
<li>只扫描一遍</li>
<li>需要密度参数作为终止条件</li>
</ul>
</li>
<li><p>参数和基本概念：</p>
<ul>
<li><p>Eps：邻域的最大半径（确定领域大小）</p>
</li>
<li><p>MinPts：邻域最大半径内的最小点数量（确定邻域最大密度）</p>
</li>
<li><p>核心对象（core object）：eps邻域内至少包含MinPts个对象（MinPts由参数给定）</p>
</li>
<li><p>直接密度可达（directly density-reachable）：p在q的eps邻域内，说明p是q直接密度可达的</p>
</li>
<li><p>密度可达的（Density-reachable）：存在对象链p1,…,pn，后一个是前一个直接密度可达的，那么说明pn是p1密度可达的；密度可达并不是一个等价关系，只有当p1,pn都是核心对象时，才一定保证可逆。</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-17/31501679.jpg" alt=""></p>
</li>
<li><p>密度相连的（Density-connected）：存在p1，p2，q，p1和q以及p2和q都是密度可达的，那么p1和p2是密度相连的。密度相连是等价关系。</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-17/63923841.jpg" alt=""></p>
</li>
</ul>
</li>
<li><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）</p>
<p>对每个核心对象，将它的所有密度可达的（但未被访问过的）对象添加到自身集合中作为它的簇。</p>
<p>未被添加的点，就是噪声</p>
</li>
</ul>
<h3 id="基于网格的方法"><a href="#基于网格的方法" class="headerlink" title="基于网格的方法"></a>基于网格的方法</h3><h3 id="聚类评估"><a href="#聚类评估" class="headerlink" title="聚类评估"></a>聚类评估</h3><ul>
<li><p>评估聚类趋势（assessing clustering tendency）</p>
<ul>
<li><p>只有对有非随机结构的数据集进行聚类，才有可能产生有意义的聚类。所以聚类要求数据的非均匀分布。</p>
</li>
<li><p>霍普金斯统计量（Hopkins Statistic）</p>
<p><img src="http://o6vut8vrh.bkt.clouddn.com/17-11-17/66309480.jpg" alt=""></p>
</li>
</ul>
</li>
<li><p>确定簇数量（determine the number of clusters）</p>
</li>
<li><p>测定聚类质量</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/blog-others/2017/11/10/数据挖掘复习/" class="article-date">
  <time datetime="2017-11-09T17:04:00.000Z" itemprop="datePublished">2017-11-10</time>
</a>

        </li>
        
          <li>
            <span class="label">Categoría:</span>
            
  <div class="article-category">
    <a class="article-category-link" href="/blog-others/categories/复习笔记/">复习笔记</a>
  </div>


          </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog-others/tags/复习笔记/">复习笔记</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog-others/tags/数据挖掘/">数据挖掘</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/blog-others/2017/11/23/机器学习的三个主要组成/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Nuevo</strong>
      <div class="article-nav-title">
        
          机器学习三要素
        
      </div>
    </a>
  
  
    <a href="/blog-others/2017/08/21/Python入门学习笔记/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Viejo</strong>
      <div class="article-nav-title">Python入门学习笔记基础</div>
    </a>
  
</nav>


  
</article>








      </div>
      
    <footer id="footer" class="post-footer footer">
      
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>nothing</p>


      </div>
    </footer>

      





<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.10/clipboard.min.js"></script>


  <link rel="stylesheet" href="/blog-others/fancybox/jquery.fancybox.css">
  <script src="/blog-others/fancybox/jquery.fancybox.pack.js"></script>


<script src="/blog-others/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
